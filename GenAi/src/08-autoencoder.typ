
= Autoencoder / Variational AE

== Autoencoder (AE)
AE = encoder + decoder trained to *reconstruct* input (output ≈ input)

*Process:*
- *Encoding:* encode data into latent space of new features
- *Sampling:* sample from distribution, decode back to original feature space
- *Decoding:* generating new samples

=== Magic of Latent Space
- Images compressed into low-dimensional latent space (embeddings)
- New images generated by sampling points in latent space
- Decoder maps latent vectors back to valid images
- Decoder learned how latent representations correspond to image structure

=== Convolution
Encoder/decoder often use convolution + (transposed conv) for down/up-sampling.

=== Activation: ReLU vs LeakyReLU
- ReLU: $f(x) = max(0, x)$ (dead neurons possible for negative region)
- LeakyReLU: small slope for $x < 0$ (keeps gradients alive)

=== Problems with Vanilla AE Latent Space
- Latent space is uneven: data clusters occupy only small regions
- Distribution is unknown and often asymmetric
- Random sampling unreliable due to unknown latent distribution
- Gaps exist: many latent points decode to invalid or poor images
- Latent space not enforced to be continuous
- Nearby latent points may decode very differently
- Higher latent dimensions amplify empty regions

=== Reconstruction Losses
- RMSE (Root Mean Squared Error): L2-style reconstruction
- Binary cross entropy: often for normalized pixel outputs; asymmetric

== Variational Autoencoder (VAE)
Instead of mapping $x$ → single latent point, map $x$ → *distribution* in latent space. Each input produces parameters of multivariate normal distribution.

=== Normal Distribution
"Normalverteilt", represented as $N$, $mu$ → mean, $sigma^2$ → variance

$ mu = (1/N) sum_(i=1)^N x_i ;  sigma^2 = (1/N) sum_(i=1)^N (x_i - mu)^2 $

Standard deviation: $sigma = sqrt(sigma^2)$

=== Encoder
Encoder takes each input image and encodes it to 2 vectors that define a multivariate normal distribution in latent space.

- Latent $"dim" = d$
- Encoder predicts:
  - mean vector: $z_"mean" in RR^d$
  - variance (via log-variance): $z_"log-var" in RR^d$
- Use log variance because variance must be positive, but log-var can be any real number

=== Reparameterization Trick
Sample a point p from normal distribution with mean $mu$ and std deviation $sigma$:
$ p = mu + sigma epsilon quad (epsilon "sampled from" N(0,I)) $

With log variance:
$ sigma = exp(0.5 times z_"log_var") $
$ z = z_"mean" + exp(0.5 times z_"log_var") times epsilon $

== VAE Loss
Total Loss = reconstruction loss + KL divergence term

- KL divergence penalizes if distribution is far from std normal distribution
- KL term pushes learned latent distributions toward standard normal $N(0, I)$ → smoother, more "fillable" latent space

=== KL (Kullback-Leibler) Divergence
$ "KL_Loss" = -0.5 times sum(1 + z_"log_var" - z_"mean"^2 - e^(z_"log_var")) $

- Sum taken over all dimensions in latent space
- kl_loss minimized to 0 when $z_"mean" = 0$ and $z_"log_var" = 0$ for all dimensions

=== Nice Properties (Intuition)
- *Sampling:* pick $z ~ N(0, 1)$ → decode → plausible outputs (less gaps than AE)
- *Smoothness:* nearby latent samples decode to similar outputs (ideally)

=== Latent Space Arithmetic / Editing
Make a sad person smile:
- Find vector in latent space pointing to direction of "smiling"
- Take average latent of smiling faces minus average latent of non-smiling faces → $z_"diff"$
- Edit: $z_"new" = z_"origin" + alpha times z_"diff"$

=== Morphing
Linear interpolation between two latent points:
$ z_"new" = z_A times (1 - alpha) + z_B times alpha $

Decode along path (each point on the way) → gradual transition from A to B